[core]
;# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
;# Set this to True if you want to enable remote logging.
;remote_logging = True
;
;# Users must supply an Airflow connection id that provides access to the storage
;# location.
;remote_log_conn_id = s3_log
;remote_base_log_folder = s3://data-airflow-logs/<folder>
;encrypt_s3_logs = False

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# Whether to load the DAG examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Whether to load the default connections that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_default_connections = False

# Secret key to save connection passwords in the db
fernet_key =  46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=

[webserver]
rbac = True
# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = True

# Number of workers to run the Gunicorn web server
workers = 2

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 1800

[celery]
# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://:redis@redis:6379/0

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres:5432/airflow

# Default queue that tasks get assigned to and that worker listen on.
default_queue = celery

[scheduler]
# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# Restart Scheduler every 41460 seconds (11 hours 31 minutes)
# The odd time is chosen so it is not always restarting on the same "hour" boundary
run_duration = 41460

[metrics]
# StatsD (https://github.com/etsy/statsd) integration settings.
# Enables sending metrics to StatsD.
statsd_on = True
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[smart_sensor]
use_smart_sensor = True
sensors_enabled = SmartFileSensor, SmartExternalTaskSensor
